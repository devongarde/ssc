processes
=========

This is how I currently imagine stuff working:


1. In comes a RESTAPI call. RestAPI does NOT define the call language. Convention is something like XML, JSON, RDFa, HTML, etc..
2. The web server receives the call and passes it on to the interpreter. It doesn't matter what web server, but I'll presume httpd for the sake of argument. The choice of passing on though does matter, and I'm thinking FastCGI.
3. The message processing layer is an interpreter running akin to a daemon. It receives the FastCGI message, and distributes it to be interpreted.
4. The interpretation layer interprets the restAPI command, and tells ssc to do it. There are questions about data held between calls, user id and so on, to be answered.
5. ssc is also running akin to a daemon, picks up the call, and does its stuff.
6. the result is reported and returned to the client
7. There is an additional monitoring daemon that makes sure the interpreter and ssc are restarted when they crash or become unresponsive

The key point is this: restAPI does not define the language it uses, one goes by convention. ssc processes and understands some of those languages, including RDFa, JSON, and HTML itself. It does NOT understand XML, and some of the other convention languages. So I have every intention of picking JSON, RDFa, or one of the other languages ssc already understands, and reusing existing code.

I regard firing up and stopping ssc each time a message comes through to be utterly inefficient. Thus it has to run permanently akin to a daemon, and respond to messages coming in on a channel TBD (as I said, I'm thinking FastCGI). A convention I've seen in this situation is to respond to text messages in a defined format. ssc is very good at responding to text in a defined format because that is what it does (it also nitpicks them). Thus I'm thinking named pipes.

The restAPI interprtation processing layer is the question. It will have to pick data out and process them, talk to the db, manage things like user info, and then send the parameters to ssc. If the call is in a language that ssc already understands, it can just pass it on. 

So, ssc has to be extended to:
- be daemonised
- add a defined interface to accept textual instructions in a format it understands, and respond
- add a mechanism TBD to report its current state (so if it fails the monitoring daemon can restart it)

The restAPI interpreation processing layer has to
- be daemonised
- add a defined interface to accept textual instructions in a restAPI format, and respond
- add a mechanism TBD to report its current state (so if it fails the monitoring daemon can restart it)
- manage the context
- talk to a db

The message processing later has to:
- be daemonised
- be highly parallel
- add a defined interface to pick up text message from the web server and it doesn't care about the format
- distribute that message to an appropriate client

Presuming the restAPI format is JSON, RDFa, and/or HTML, then the restAPI interpretator and ssc do the same thing.

The question is the parallelisation. ssc as designed now will only be able to support one request at a time. To support multiple concurrently, it has to be run concurrently.

Form the perspective of here, only one web server will be running, serving lots of requests concurrently. Whether there is one message processor or lots of message processors running really depends on the capability of the web server and the communication mechanism chosen. I suspect it should also be highly parallel.

If we put the restAPI interpretater in the message processing layer it has to be highly parallel, and has to repeat existing code. If we put it in ssc, we reuse existing code, and the message processing layer simply manages the distribution of tasks.

As an aside, I think the database solution has to be highly distributed and fast, so I would lean to something like Redis, not SQL server.

It's worth mention I don't like being trapped to a particular supplier or a particular product. I want the option to switch to another should something silly happen.



goals
=====
show off ssc and offer it as a service

need to hold user information and check that everything is valid, but user details help by hosting environment
want users to be validated

overall
- users can use ssc as soas to nitpick static websites
- offer web interface and API
- performant -ish
- freebie users can try stuff out but only on sites they control, or specific demo sites

freebie users cannot cost, which means I think:
- no data storage except absolutely necessary then that's transitory
- results cannot be stored, they are delivered immediately
- pretty severe restrictions on network traffic use
- they have to control the sites they scan (might be worth setting up a special test site they can scan too)
- do they need to be identified by the host environment? Makes an upgrade far easier if that's so.

pay users:
- identified by the host environment, not just for dosh management
- pay for traffic and storage and some, storage is, if possible, under their direct control
- can use all features of ssc including cache data between runs and collect sanitised version of site (tho' all may not be available immediately)
- results are either delivered immediately or stored for collection

other stuff:
- keep track of everything that happens, e.g. log stuff
- keep a special eye on errors so problems can be reproduced, identified & fixed
- make sure everyone knows commands and results will be stored for analysis with id info stripped out, although pay users can drop out of this

third party sites
- limits on third party site scanning, perhaps one site per pay user per month
- need a mechanism to allow third party site owners to control access, probably easiest via robots.txt (ignored if site ownership file present)



data
====
all data held internal to product, except pay user output to pay user blobs/filesystems
what happens if data nicked, thus SECURITY NEEDS TO BE CONSIDERED NOW ... I think encrypt everything held internally
not everything gets implemented immediately!!!
date means pretty precise datetime


user
----
internal user id (probably nothing more than an incremented integer)
reference to cloud user stuff (as supplied by host environment)
user's chosen name
user's chosen email address (unless cloud requires a specific address)
has active account (presumes account info / money managed by host environment)
user wishes to suppress report once it is delivered (otherwise might keep it for analysis on error)

session
-------
user link
randomly generated session id (GUID?), maybe each reply supplies a new session id to confuse replay attacks (e.g. TBD)
session info (dunno what that is yet)
date last activity

log
---
date
user link
what (joined, validated, subscribed, unsubscribed, deleted, site added, site removed, reported, cleaned)
site link
report link
comment

domain
------
DNS name
unique id (GUID) generated by this system. GUID used because id needs to be unique across the internet, as much as possible

site
----
domain link
full address
protocol (e.g. http or https)
unique id filename (e.g. ssc-GUID.html), contains domain unique id. (Filename not path because must be in site root to ensure user has control of site, not just part of it.)
default parameters for generating report?

user site
---------
user link
site link
date last scan

store
-----
user site link
type, either ndx (cache), generated site, report, maybe more
host system identifier (to keep multiple copies, use versioned file system)

report
------
site link
session link
parameters used to generate report
content (can be gigabytes) (probably a link, which might be specified by the host environment, perhaps a user cloud blob)
volume (bytes) (non-paying users have a maximum volume)
error link, if applicable
can either returned via restAPI (maybe truncated), emailed to user (maybe truncated), andor output to pay user's blob

errors
------
report link
error info
date
precise ssc version
any relevant info



data security
=============
What needs to be secured?
Absolutely User identification information
what about site information?
what about everything but big data, & dump big data on user controlled blobs/filesystems?
comms between client and web server should be identified only by session id, and absolutely not any user info
comms requires https or better; a connection by http will return 418 to tell the caller not to be so silly


RestAPI Parameters
==================

most content as per ssc config file, but that's in INI file format which isn't acceptable, so will have to be adapted. Maybe accept any currently understood file format.
additional content as required by restAPI, such as session info
Format RDFa, JSON, or what? Must be format already understood by ssc so product can return snarky messages when it's erroneous


API?
====
- new session
- call per ssc parameter
- new parameters include maximum output, where to send feedback????
- go
- get current state (what is state? current stage? Percentage done? Files being processed?)
- finish



Processes
=========
web server (both web site and receiving api calls)
message processing layer (fastcgi)
api processing layer (many processes, one per request, talks to db)
ssc (many processes, one per request)
health process (can send message to web server so api layer logs crashes)

undecided whether api and ssc are same or separate processes


To Do
=====
finish design
Design and implement API
hook api into ssc
Set up host environment and integrate into it
Set up website
finish css (out of scope here though)

